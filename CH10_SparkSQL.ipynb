{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10장. 스파크SQL\n",
    "\n",
    "- SQL 구조적 질의 언어 : 데이터에 대한 관계형 연산을 표현하기 위한 도메인 특화 언어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 빅데이터와 SQL : 아파치 하이브\n",
    "- 스파크가 등장하기 전에는 하이브가 빅데이터SQL 접근계층에서 사실상 표준\n",
    "- 하둡을 다양한 산업군으로 진출시키는데 큰 역할을 함\n",
    "\n",
    "#### 빅데이터와 SQL : 스파크SQL\n",
    "- ANSI-SQL과 HiveQL을 모두 지원\n",
    "- 쓰리프트서버(JDBC ODBC :관계형데이터베이스 표준 접속 프로토콜)나 SQL인터페이스에 접속해 스파크 연산 능력활용 가능\n",
    "    - 통합API 덕분에 데이터를 DataFrame으로 변환\n",
    "    - 스파크 MLlib이 제공하는 대규모 머신러닝 알고리즘 수행\n",
    "    - 결과를 다른 데이터소스에 저장\n",
    "\n",
    "##### 스파크와 하이브의 관계\n",
    "- 스파크SQL은 조회할 파일 수를 최소화하기 위해 하이브 메타스토어를 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 스파크SQL 쿼리 실행 방법\n",
    "#### 스파크SQL CLI\n",
    "- ./bin/spark-sql  : CLI형태의 SQL이 실행됨\n",
    "-  CLI는 쓰리프트서버와 통신 불가능\n",
    "    \n",
    "#### 스파크 프로그래밍 SQL인터페이스 \n",
    "- SparkSession의 sql 메서드를 활용\n",
    "- 다른 트랜스포메이션과 마찬가지로 지연 처리됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|   DEST_COUNTRY_NAME|sum(count)|\n",
      "+--------------------+----------+\n",
      "|             Senegal|        40|\n",
      "|              Sweden|       118|\n",
      "|               Spain|       420|\n",
      "|    Saint Barthelemy|        39|\n",
      "|Saint Kitts and N...|       139|\n",
      "|         South Korea|      1048|\n",
      "|        Sint Maarten|       325|\n",
      "|        Saudi Arabia|        83|\n",
      "|         Switzerland|       294|\n",
      "|         Saint Lucia|       123|\n",
      "|               Samoa|        25|\n",
      "|        South Africa|        36|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.json(\"file:///home/ubuntu/ybigta/Dataset_spark/data/flight-data/json/2015-summary.json\")\\\n",
    "        .createOrReplaceTempView(\"some_sql_view\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count) FROM some_sql_view GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\").where(\"DEST_COUNTRY_NAME like 'S%'\").where(\"`sum(count)` > 10\")\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 스파크SQL 쓰리프트 JDBC / ODBC 서버\n",
    "- 하이브1.2.1 버전의 beeline스크립트 혹은 스파크로 테스트해볼 수 있다.\n",
    "    - JDBC / ODBC 서버 시작\n",
    "        - 기본적으로 localhost:10000 주소를 사용함\n",
    "        - ./sbin/start-thriftserver.sh\n",
    "    - 접속\n",
    "        - ./bin/beeline\n",
    "        - beeline> !connect jdbc:hive2://localhost:10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 카탈로그\n",
    "- 테이블에 저장된 데이터에 대한 메타데이터, 데이터베이스, 테이블, 함수, 뷰에 관한 정보를 추상화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 테이블\n",
    "#### 관리형테이블과 외부테이블\n",
    "- 관리형테이블\n",
    "    - 테이블의 데이터와 테이블에 대한 데이터(메타데이터) 모두 추적하는 테이블\n",
    "    - saveAsTable 메서드로 관리형 테이블 생성 가능 > 하이브의 기본 웨어하우스 경로(/user/hive/warehouse)에 저장\n",
    "\n",
    "- 외부테이블 \n",
    "    - 테이블의 메타데이터를 관리, 테이블의 데이터는 스파크에서 관리하지 않음\n",
    "    - 디스크에 저장된 파일을 이용해 테이블을 정의하면 외부테이블을 작성하는것!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 테이블의 생성\n",
    "- 다양한 데이터소스를 활용해 테이블을 생성할 수 있다.\n",
    "- comment도 달수 있다.\n",
    "- USING을 이용하지 않으면 스파크는 기본적으로 하이브SerDe 설정을 사용하는데 이는 매우 느림..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS flights (DEST_COUNTRY_NAME STRING COMMENT 'we can set comment of column', ORIGIN_COUNTRY_NAME STRING, count LONG)\n",
    "USING JSON OPTIONS (path 'file:///home/ubuntu/ybigta/Dataset_spark/data/flight-data/json/2015-summary.json')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SELECT 쿼리를 이용해 테이블을 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# USING을 이용해 포맷을 지정하지 않아 하이브 호환 테이블을 만듬\n",
    "spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS flights_from_select USING parquet AS SELECT * FROM flights\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 파티셔닝된 데이터셋을 저장해 데이터 레이아웃을 제어할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE partitioned_flights USING parquet PARTITIONED BY (DEST_COUNTRY_NAME) AS \n",
    "SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count FROM flights LIMIT 5\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 외부테이블 생성하기\n",
    "- 스파크는 외부테이블의 메타데이터를 관리\n",
    "- CREATE EXTERNAL TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE EXTERNAL TABLE hive_flights (DEST_COUNTRY_NAME STRING COMMENT 'we can set comment of column', ORIGIN_COUNTRY_NAME STRING, count LONG)\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION 'file:///home/ubuntu/ybigta/Dataset_spark/data/flight-data-hive/'\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SELECT 쿼리를 이용해 외부테이블 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE EXTERNAL TABLE hive_flights_2 ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \n",
    "LOCATION 'file:///home/ubuntu/ybigta/Dataset_spark/data/flight-data-hive/' AS SELECT * FROM flights\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 테이블에 데이터 삽입하기\n",
    "- 표준 SQL문법을 따름 \n",
    "- INSERT 문법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flights_from_select 테이블에 flights테이블의 20개의 데이터를 삽입\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO flights_from_select SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count FROM flights LIMIT 20\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 특정 파티션에만 저장할 경우 파티션 명세를 추가하면됨\n",
    "- 쓰기연산은 파티셔닝 스키마에 맞게 데이터를 저장\n",
    "- PARTITION (조건) 구문 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "INSERT INTO partitioned_flights\n",
    "PARTITION (DEST_COUNTRY_NAME='UNITED STATES') \n",
    "SELECT count, ORIGIN_COUNTRY_NAME FROM flights \n",
    "WHERE DEST_COUNTRY_NAME='UNITED STATES' LIMIT 12\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 테이블의 메타데이터 확인하기\n",
    "- DESCRIBE 구문은 메타데이터 정보를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+-------+\n",
      "|           col_name|data_type|comment|\n",
      "+-------------------+---------+-------+\n",
      "|  DEST_COUNTRY_NAME|   string|   null|\n",
      "|ORIGIN_COUNTRY_NAME|   string|   null|\n",
      "|              count|   bigint|   null|\n",
      "+-------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "DESCRIBE TABLE flights\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SHOW PARTITIONS 로 파티셔닝 스키마 정보를 볼수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|partition                      |\n",
      "+-------------------------------+\n",
      "|DEST_COUNTRY_NAME=Egypt        |\n",
      "|DEST_COUNTRY_NAME=United States|\n",
      "+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SHOW PARTITIONS partitioned_flights\"\"\").show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 테이블 메타데이터 갱신하기\n",
    "- 가장 최신의 데이터셋을 읽고 있다는 것을 보장할 수 있는 중요한 작업!!!\n",
    "- REFRESH TABLE 구문으로 캐싱된 모든 항목을 갱신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "REFRESH TABLE partitioned_flights\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 카탈로그에서 관리하는 테이블의 파티션 정보를 새로 고치는 REPAIR TABLE\n",
    "- 새로운 파티션 정보를 수집하는데 초점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "MSCK REPAIR TABLE partitioned_flights\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 테이블 제거하기\n",
    "- 테이블은 DELETE가 아니라 DROP 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "DROP TABLE IF EXISTS flights_csv\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 외부테이블을 제거하면 데이터 삭제는 이뤄지지 않지만 더이상 검색 불가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 테이블 캐싱하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#캐싱\n",
    "spark.sql(\"\"\"\n",
    "CACHE TABLE flights\"\"\")\n",
    "# 캐싱제거\n",
    "spark.sql(\"\"\"\n",
    "UNCACHE TABLE flights\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 뷰 VIEW\n",
    "- 뷰는 단순 쿼리 실행 계획\n",
    "- 쿼리로직을 체계화, 재사용하기 편하게 만들 수 있음\n",
    "- 뷰는 실질적으로 트랜스포메이션이며 스파크는 쿼리가 실행될 때만 뷰에 정의된 트랜스포메이션을 수행\n",
    "- 뷰는 테이블처럼 쓸수 있음\n",
    "    - 전역 뷰\n",
    "    - 세션별 뷰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 뷰 생성하기\n",
    "- 데이터를 저장하는 것이 아니라 쿼리시점에 트랜스포메이션(filter, select, groupby rollup)을 수행\n",
    "- CREATE VIEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE VIEW just_usa_view AS SELECT * FROM flights WHERE dest_country_name = 'United States'\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 임시 뷰: 테이블과 달리 데이터베이스에 등록되지 않고 현재 세션에서만 사용할 수 있음\n",
    "- CREATE TEMP VIEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TEMP VIEW just_usa_view AS SELECT * FROM flights WHERE dest_country_name = 'United States'\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 전역적 임시 뷰 : 데이터베이스에 상관없이 사용할 수 있어 전체 스파크 애플리케이션에서 볼 수 있다.\n",
    "- CREATE GLOBAL TEMP VIEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+\n",
      "|database|          tableName|isTemporary|\n",
      "+--------+-------------------+-----------+\n",
      "| default|            flights|      false|\n",
      "| default|flights_from_select|      false|\n",
      "| default|       hive_flights|      false|\n",
      "| default|     hive_flights_2|      false|\n",
      "| default|      just_usa_view|      false|\n",
      "| default|partitioned_flights|      false|\n",
      "|        |      just_usa_view|       true|\n",
      "|        |      some_sql_view|       true|\n",
      "+--------+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark.sql(\"\"\"\n",
    "# CREATE GLOBAL TEMP VIEW just_usa_view AS\n",
    "# SELECT * FROM flights WHERE dest_country_name = 'United States'\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SHOW TABLES\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 뷰 덮어 쓰기!\n",
    "- CREATE OR REPLACE TEMP VIEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW just_usa_view AS\n",
    "SELECT * FROM flights WHERE dest_country_name = 'United States'\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- EXPLAIN 구문으로 물리적 실행계획을 확인 할 수 있다.\n",
    "- 파이썬, 스칼라에서도 explain 속성으로 실행계획을 볼 수 있따."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|plan                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|== Physical Plan ==\n",
      "*(1) Project [DEST_COUNTRY_NAME#32, ORIGIN_COUNTRY_NAME#33, count#34L]\n",
      "+- *(1) Filter (isnotnull(dest_country_name#32) && (dest_country_name#32 = United States))\n",
      "   +- *(1) FileScan json default.flights[DEST_COUNTRY_NAME#32,ORIGIN_COUNTRY_NAME#33,count#34L] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/home/ubuntu/ybigta/Dataset_spark/data/flight-data/json/2015-summary.json], PartitionFilters: [], PushedFilters: [IsNotNull(DEST_COUNTRY_NAME), EqualTo(DEST_COUNTRY_NAME,United States)], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:bigint>|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "EXPLAIN SELECT * FROM just_usa_view\"\"\").show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 뷰 제거하기\n",
    "- 테이블과 제거 방식 동일 DROP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+\n",
      "|database|          tableName|isTemporary|\n",
      "+--------+-------------------+-----------+\n",
      "| default|            flights|      false|\n",
      "| default|flights_from_select|      false|\n",
      "| default|       hive_flights|      false|\n",
      "| default|     hive_flights_2|      false|\n",
      "| default|partitioned_flights|      false|\n",
      "|        |      some_sql_view|       true|\n",
      "+--------+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "DROP VIEW IF EXISTS just_usa_view\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SHOW TABLES\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 데이터베이스\n",
    "- 데이터베이스 : 여러테이블을 조직화하기 위한 도구 (지정하지 않으면 스파크는 default 데이터베이스 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|default     |\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SHOW DATABASES\"\"\").show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터베이스 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE DATABASE seungsu_db\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사용할 데이터베이스 설정하기\n",
    "- 지정한 데이터베이스를 참조하기에 다른 데이터베이스를 참조하면 쿼리가 실패"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|seungsu_db        |\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "USE seungsu_db\"\"\")\n",
    "\n",
    "# 사용중 데이터베이스 확인 구문\n",
    "spark.sql(\"\"\"\n",
    "SELECT current_database()\"\"\").show(1, False)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "USE default\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 올바른 접두사를 붙이면 명확한 테이블 지정이 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|   15|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Ireland|  344|\n",
      "|               Egypt|      United States|   15|\n",
      "|       United States|              India|   62|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|            Grenada|   62|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|       United States|       Sint Maarten|  325|\n",
      "|       United States|   Marshall Islands|   39|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|       United States|           Paraguay|    6|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|       United States|          Gibraltar|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM default.flights\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터베이스 제거하기\n",
    "- 테이블과 뷰 제거와 동일 DROP!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "DROP DATABASE IF EXISTS seungsu_db\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"SHOW DATABASES\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.6 SELECT 구문\n",
    "- 기존의 ANSI_SQL문법을 따름"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.7 고급주제 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 복합데이터 타입!\n",
    "- 복합데이터 타입은 표준 SQL에는 없지만 매우 강력한 기능! 스파크에서는 사용할 수 있다구~\n",
    "\n",
    "##### 구조체\n",
    "- 구조체를 만들기 위해서는 ()로 묶으면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----+\n",
      "|country                 |count|\n",
      "+------------------------+-----+\n",
      "|[United States, Romania]|15   |\n",
      "|[United States, Croatia]|1    |\n",
      "|[United States, Ireland]|344  |\n",
      "|[Egypt, United States]  |15   |\n",
      "|[United States, India]  |62   |\n",
      "+------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |15   |\n",
      "|United States    |Croatia            |1    |\n",
      "|United States    |Ireland            |344  |\n",
      "|Egypt            |United States      |15   |\n",
      "|United States    |India              |62   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |15   |\n",
      "|United States    |Croatia            |1    |\n",
      "|United States    |Ireland            |344  |\n",
      "|Egypt            |United States      |15   |\n",
      "|United States    |India              |62   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 생성\n",
    "spark.sql(\"\"\"\n",
    "CREATE VIEW IF NOT EXISTS nested_data AS \n",
    "SELECT (DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME) as country, count FROM flights\"\"\")\n",
    "\n",
    "# 조회        띠용~!!~!~!~!~!\n",
    "spark.sql(\"\"\"\n",
    "SELECT * FROM nested_data\"\"\").show(5, False)\n",
    "spark.sql(\"\"\"\n",
    "SELECT country.DEST_COUNTRY_NAME, country.ORIGIN_COUNTRY_NAME, count FROM nested_data\"\"\").show(5, False)\n",
    "spark.sql(\"\"\"\n",
    "SELECT country.*, count FROM nested_data\"\"\").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 리스트\n",
    "- collect_list , collect_set >> 집계함수!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+---------------+\n",
      "|new_name|flight_counts|     origin_set|\n",
      "+--------+-------------+---------------+\n",
      "|Anguilla|         [41]|[United States]|\n",
      "|Paraguay|         [60]|[United States]|\n",
      "|  Russia|        [176]|[United States]|\n",
      "| Senegal|         [40]|[United States]|\n",
      "|  Sweden|        [118]|[United States]|\n",
      "+--------+-------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+--------------+\n",
      "|DEST_COUNTRY_NAME|array(1, 2, 3)|\n",
      "+-----------------+--------------+\n",
      "|    United States|     [1, 2, 3]|\n",
      "|    United States|     [1, 2, 3]|\n",
      "|    United States|     [1, 2, 3]|\n",
      "|            Egypt|     [1, 2, 3]|\n",
      "|    United States|     [1, 2, 3]|\n",
      "+-----------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+----------------------+\n",
      "|new_name|collect_list(count)[0]|\n",
      "+--------+----------------------+\n",
      "|Anguilla|                    41|\n",
      "|Paraguay|                    60|\n",
      "|  Russia|                   176|\n",
      "| Senegal|                    40|\n",
      "|  Sweden|                   118|\n",
      "+--------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 생성\n",
    "spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME as new_name, collect_list(count) as flight_counts, \n",
    "    collect_set(ORIGIN_COUNTRY_NAME) as origin_set\n",
    "    FROM flights GROUP BY DEST_COUNTRY_NAME\"\"\").show(5)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, ARRAY(1,2,3) FROM flights\"\"\").show(5)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME as new_name, collect_list(count)[0] FROM flights GROUP BY DEST_COUNTRY_NAME\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- explode 함수는 collect계열과 반대로 작용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+\n",
      "|collected_counts|DEST_COUNTRY_NAME|\n",
      "+----------------+-----------------+\n",
      "|            [41]|         Anguilla|\n",
      "|            [60]|         Paraguay|\n",
      "|           [176]|           Russia|\n",
      "|            [40]|          Senegal|\n",
      "|           [118]|           Sweden|\n",
      "+----------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+-----------------+\n",
      "|col|DEST_COUNTRY_NAME|\n",
      "+---+-----------------+\n",
      "| 41|         Anguilla|\n",
      "| 60|         Paraguay|\n",
      "|176|           Russia|\n",
      "| 40|          Senegal|\n",
      "|118|           Sweden|\n",
      "+---+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW flights_add AS SELECT DEST_COUNTRY_NAME, collect_list(count) as collected_counts\n",
    "FROM flights GROUP BY DEST_COUNTRY_NAME\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT collected_counts, DEST_COUNTRY_NAME FROM flights_add\"\"\").show(5)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT explode(collected_counts), DEST_COUNTRY_NAME FROM flights_add\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 함수\n",
    "- 함수목록 확인 : SHOW FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|function|\n",
      "+--------+\n",
      "|       !|\n",
      "|       %|\n",
      "|       &|\n",
      "|       *|\n",
      "|       +|\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+\n",
      "|function|\n",
      "+--------+\n",
      "|       !|\n",
      "|       %|\n",
      "|       &|\n",
      "|       *|\n",
      "|       +|\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+\n",
      "|function|\n",
      "+--------+\n",
      "+--------+\n",
      "\n",
      "+---------+\n",
      "| function|\n",
      "+---------+\n",
      "|   second|\n",
      "|sentences|\n",
      "|      sha|\n",
      "|     sha1|\n",
      "|     sha2|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SHOW FUNCTIONS\"\"\").show(5)\n",
    "\n",
    "spark.sql(\"\"\"SHOW SYSTEM FUNCTIONS\"\"\").show(5)\n",
    "\n",
    "spark.sql(\"\"\"SHOW USER FUNCTIONS\"\"\").show(5)\n",
    "\n",
    "spark.sql(\"\"\"SHOW FUNCTIONS 's*'\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 사용자 정의 함수\n",
    "    - 하이브의 CREATE TEMPORARY FUNCTION 구문으로 등록가능\n",
    "    - pyspark 의 udf 활용\n",
    "    - 6.10장 참고!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 서브쿼리\n",
    "- 비상호연관 서브쿼리 : 정보를 보완하기 위해 쿼리의 외부범위에 있는 일부 정보 사용가능\n",
    "- 상호연관 서브쿼리 : 외부범위의 정보를 사용 불가\n",
    "- 조건절 서브쿼리 : 값에 따라 필터링 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 비상호연관 조건절 서브쿼리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|dest_country_name|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|           Canada|\n",
      "|           Mexico|\n",
      "|   United Kingdom|\n",
      "|            Japan|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|            Egypt|      United States|   15|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "|           Guyana|      United States|   64|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT dest_country_name FROM flights GROUP BY dest_country_name ORDER BY sum(count) DESC LIMIT 5\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"SELECT * FROM flights\n",
    "WHERE origin_country_name IN (SELECT dest_country_name FROM flights \n",
    "GROUP BY dest_country_name ORDER BY sum(count) DESC LIMIT 5)\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 상호연관 조건절 서브쿼리 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |15   |\n",
      "|United States    |Croatia            |1    |\n",
      "|United States    |Ireland            |344  |\n",
      "|Egypt            |United States      |15   |\n",
      "|United States    |India              |62   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 왕복 비행기가 있는지\n",
    "spark.sql(\"\"\"\n",
    "SELECT * FROM flights f1 \n",
    "WHERE EXISTS (SELECT 1 FROM flights f2 WHERE f1.dest_country_name = f2.origin_country_name)\n",
    "AND EXISTS (SELECT 1 FROM flights f2 WHERE f2.dest_country_name = f1.origin_country_name)\"\"\").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 비상호연관 스칼라 쿼리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|maximum|\n",
      "+-----------------+-------------------+-----+-------+\n",
      "|United States    |Romania            |15   |370002 |\n",
      "|United States    |Croatia            |1    |370002 |\n",
      "|United States    |Ireland            |344  |370002 |\n",
      "|Egypt            |United States      |15   |370002 |\n",
      "|United States    |India              |62   |370002 |\n",
      "+-----------------+-------------------+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 하나의 컬럼을 추가하고 싶을때\n",
    "spark.sql(\"\"\"\n",
    "SELECT *, (SELECT max(count) FROM flights) AS maximum FROM flights\"\"\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
