{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7장. 집계함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"file:///home/ubuntu/ybigta/Dataset_spark/data/retail-data/all/*.csv\")\\\n",
    "    .coalesce(5)  \n",
    "# coalesce : null 값 거르기\n",
    "    \n",
    "df.cache()       # 빠른 접근을 위해\n",
    "df.createOrReplaceTempView(\"dfTable\")\n",
    "# 스키마 정보 출력\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모든 집계함수는 함수를 사용하거나 DataFrame의 stat 속성을 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 > 집계함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- count : 전체 레코드 수를 구할때\n",
    "- countDistinct : 고유 레코드 수를 구할때\n",
    "- approx_count_distinct : 레코드 수 근사치 구하기~\n",
    "- first / last : DataFrame의 첫번째 값 / 마지막 값\n",
    "- min / max : 최소 / 최대\n",
    "- sum : 합\n",
    "- sumDistinct\n",
    "- 평균 avg\n",
    "- 분산과 표준편차 variance / stddev / var_pop / stddev_pop\n",
    "- 비대칭도와 첨도(변곡점) skewness\n",
    "- 공분산과 상관관계 covar_samp / covar_pop / corr\n",
    "- 복합데이터 타입의 집계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n",
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n",
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, countDistinct, approx_count_distinct\n",
    "\n",
    "df.select(count(\"StockCode\")).show()\n",
    "\n",
    "df.select(countDistinct(\"StockCode\")).show()\n",
    "\n",
    "df.select(approx_count_distinct(\"StockCode\", 0.1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------------+\n",
      "|first(StockCode, false)|last(StockCode, false)|\n",
      "+-----------------------+----------------------+\n",
      "|                 85123A|                 22138|\n",
      "+-----------------------+----------------------+\n",
      "\n",
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n",
      "+-------------+----------------------+\n",
      "|sum(Quantity)|sum(DISTINCT Quantity)|\n",
      "+-------------+----------------------+\n",
      "|      5176450|                 29310|\n",
      "+-------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, last, min, max, sum, sumDistinct\n",
    "\n",
    "df.select(first(\"StockCode\"), last(\"StockCode\")).show()\n",
    "\n",
    "df.select(min(\"Quantity\"), max(\"Quantity\")).show()\n",
    "\n",
    "df.select(sum(\"Quantity\"), sumDistinct(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+----------------+\n",
      "|       sum/count|   avg_purchases|  mean_purchases|\n",
      "+----------------+----------------+----------------+\n",
      "|9.55224954743324|9.55224954743324|9.55224954743324|\n",
      "+----------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, avg, expr\n",
    "\n",
    "df.select(\n",
    "count(\"Quantity\").alias(\"total_transaction\"),\n",
    "sum(\"Quantity\").alias(\"total_purchases\"),\n",
    "avg(\"Quantity\").alias(\"avg_purchases\"),\n",
    "expr(\"mean(Quantity)\").alias(\"mean_purchases\"))\\\n",
    ".selectExpr(\n",
    "\"total_purchases/total_transaction as `sum/count`\",\n",
    "\"avg_purchases\",\n",
    "\"mean_purchases\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 분산, 표준편차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+---------------------+\n",
      "| var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|\n",
      "+------------------+------------------+--------------------+---------------------+\n",
      "|47559.303646609354| 47559.39140929905|  218.08095663447864|   218.08115785023486|\n",
      "+------------------+------------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import var_pop, stddev_pop          #모집단 분산/표준편차\n",
    "from pyspark.sql.functions import var_samp, stddev_samp        #표본집단 분산/표준편차\n",
    "\n",
    "df.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"), stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 비대칭도와 첨도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|  skewness(Quantity)|kurtosis(Quantity)|\n",
      "+--------------------+------------------+\n",
      "|-0.26407557610527843|119768.05495536518|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import skewness, kurtosis\n",
    "\n",
    "df.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 공분산과 상관관계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|corr(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|     4.912186085637639E-4|             1052.7280543913773|            1052.7260778752732|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr, covar_pop, covar_samp\n",
    "\n",
    "df.select(corr(\"InvoiceNo\", \"Quantity\"), covar_samp(\"InvoiceNo\", \"Quantity\"), \n",
    "         covar_pop(\"InvoiceNo\", \"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 복합데이터 다루기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|collect_set(Country)|collect_list(Country)|\n",
      "+--------------------+---------------------+\n",
      "|[Portugal, Italy,...| [United Kingdom, ...|\n",
      "+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set, collect_list\n",
    "\n",
    "df.agg(collect_set(\"Country\"), collect_list(\"Country\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
