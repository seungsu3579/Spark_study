{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9장. 데이터소스\n",
    "- CSV\n",
    "- JSON\n",
    "- 파케이\n",
    "- ORC\n",
    "- JDBC / ODBC 연결\n",
    "- 일반 텍스트 파일\n",
    "- 그외 다양한 데이터소스가 존재 \n",
    "    - XML\n",
    "    - MONGO DB\n",
    "    - HBase\n",
    "    - 카산드라\n",
    "    - AWS Redshift\n",
    "    - ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 > 데이터소스의 API구조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 읽기의 기초\n",
    "- 데이터를 읽을 때 DataFrameReader 클래스 사용\n",
    "- SparkSession의 read 속성으로 접근"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.readwriter.DataFrameReader at 0x7f4d29028278>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DataFrameReader는 포맷, 스키마, 읽기모드, 옵션을 지정해야함\n",
    "    - 설정값을 가진 Map 객체를 전달할 수도 있음\n",
    "    <br><br>\n",
    "    - format(선택) : (default = 파케이)\n",
    "    - option : 데이터를 읽는 방법에 대한 파라미터를 key, value 쌍으로 설정 가능\n",
    "    - 읽기모드 \n",
    "        - permissive (default) : 오류가 있는 레코드의 모든 필드를 null로 채움, _corrupt_record 컬럼에 기록\n",
    "        - dropMalformed : 형식에 맞지 않는 레코드를 제거(drop)\n",
    "        - failfast : 형식에 맞지 않는 레코드를 만나면 종료 : 안읽고 말지~\n",
    "    - schema : 데이터소스에서 스키마를 제공, 스키마 추론 기능 사용가능 : 포멧별로 옵션이 다름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.read.format(\"csv\")\\                     << 포맷\n",
    "#         .option(\"mode\", \"FAILFAST\")\\          << 읽기모드\n",
    "#         .option(\"inferSchema\", \"true\")\\\n",
    "#         .schema(someSchema)\\                  << 스키마 정보\n",
    "#         .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 쓰기의 기초\n",
    "- 데이터를 쓸 때 DataFrameWriter 클래스 사용\n",
    "- DataFrame의 write 속성으로 접근 ( DataFrame별로 접근)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.readwriter.DataFrameWriter at 0x7f4d2902e278>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame([()]).write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DataFrameWriter는 포맷, 저장모드, 옵션, 데이터가 저장될 경로를 지정해야함\n",
    "    <br><br>\n",
    "    - format(선택) : (default = 파케이)\n",
    "    - option : 데이터를 읽는 방법에 대한 파라미터를 key, value 쌍으로 설정 가능\n",
    "    - 읽기모드 \n",
    "        - append : 해당 경로에 이미 존재하는 파일 목록에 결과 파일을 추가\n",
    "        - overwrite : 이미 존재하는 모든 데이터를 덮어씀\n",
    "        - errorIfExists(default) : 해당 경로에 이미 데이터나 파일이 존재하면 종료 : 쓰기작업 실패!\n",
    "        - ignore : 해당 경로에 데이터나 파일이 존재하면 아무런 처리하지 않음\n",
    "    - 기타 메서드 : 파일기반의 데이터소스에서 작동\n",
    "        - partitionBy \n",
    "        - bucketBy\n",
    "        - sortBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataFrame.write.format(\"csv\")\\                << 포맷\n",
    "#         .option(\"mode\", \"OVERWRITE\")\\         << 읽기모드\n",
    "#         .option(\"dataFormat\", \"yyyy-MM-dd\")\\\n",
    "#         .option(\"path\", \"path/to/file(s)\")\\   << 파일경로\n",
    "#         .partitionBy(...)                     << 기타 메서드\n",
    "#         .bucketBy(...)\n",
    "#         .sortBy(...)\n",
    "#         .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 > CSV파일\n",
    "- , 로 value가 구분되어져 있는 파일\n",
    "- 각 줄이 하나의 레코드가 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV 파일 읽기\n",
    "- 스키마를 주어짐으로서 기대했던 데이터 포맷을 얻을 수 있다.\n",
    "- 읽어 들인 후 데이터 타입의 변경이 이뤄지면 당시에는 모르지만 데이터를 읽어들일 때 에러 발생\n",
    "    - 지연 연산때문...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrameReader' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-71cf9760d45d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcsvFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mode\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"FAILFAST\"\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inferSchema\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"file:///home/ubuntu/ybigta/Dataset/sdfsd,csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#     .load(\"file:///home/ubuntu/ybigta/Dataset_spark/data/flight-data/csv/2010-summary.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# csvFile.show(5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrameReader' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "csvFile = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"mode\", \"FAILFAST\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"path\", \"file:///home/ubuntu/ybigta/Dataset/sdfsd,csv\")\n",
    "#     .load(\"file:///home/ubuntu/ybigta/Dataset_spark/data/flight-data/csv/2010-summary.csv\")\n",
    "# csvFile.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV 파일 쓰기\n",
    "- maxColumns, inferSchema를 제외한 옵션 읽기와 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV파일을 읽고 TSV파일로 변환하여 저장\n",
    "csvFile.write.format(\"csv\").option(\"mode\", \"OVERWRITE\").option(\"sep\", \"\\t\")\\\n",
    ".save(\"file:///home/ubuntu/ybigta/Dataset_spark/tmp/my-tsv-file.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 > JSON파일\n",
    "- JSON은 javascript의 객체 파일이기에 CSV보다 적은 옵션을 가짐\n",
    "- multiLine 옵션을 통해 전체파일을 하나의 JSON객체로 읽을 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonFile = spark.read.format(\"json\")\\\n",
    "    .option(\"mode\", \"FAILFAST\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"file:///home/ubuntu/ybigta/Dataset_spark/data/flight-data/json/2010-summary.json\")\n",
    "csvFile.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON 파일 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV파일 JSON으로 저장하기~\n",
    "csvFile.write.format(\"json\").mode(\"overwrite\")\\\n",
    ".save(\"file:///home/ubuntu/ybigta/Dataset_spark/tmp/my-json-file.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 > 파케이(parquet) 파일\n",
    "- 분석 워크로드에 최적화되어 있음\n",
    "- 컬럼기반 데이터 저장 방식\n",
    "- 복합데이터 타입 지원\n",
    "- 전체 파일을 읽는 대신 개별 컬럼을 읽을 수 있음, 컬럼기반 압축기능까지\n",
    "- 아파치 스파크와 호환이 잘됨\n",
    "- 장기 저장용 데이터는 파케이 포맷이 좋음~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PARQUET 파일 읽기\n",
    "- 정확한 스키마가 필요할때만 스키마 설정! 스키마가 내장되어있기 때문!\n",
    "\n",
    "- 데이터 저장시 스키마를 통해 데이터 저장 >> 옵션이 거의 없음! 포맷만으로도 충분          : 너무 좋자나~\n",
    "    - 파케이 파일 옵션 (단 2개!)\n",
    "        - compression / codec : 스파크가 파일을 읽고 쓸때 사용하는 압축 코덱\n",
    "        - mergeSchema : 동일한 테이블, 폴더에 신규추가된 파에키파일에 컬럼을 추가하는 기능을 쓸지 말지(T/F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquetFile = spark.read.format(\"parquet\")\\\n",
    "        .load(\"file:///home/ubuntu/ybigta/Dataset_spark/data/flight-data/parquet/2010-summary.parquet\")\n",
    "parquetFile.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PARQUET 파일 쓰기\n",
    "- 경로만 명시하면 됨!<pre>최고다 파케이!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV파일을 PARQUET파일로 변환하기\n",
    "csvFile.write.format(\"parquet\").mode(\"overwrite\")\\\n",
    ".save(\"file:///home/ubuntu/ybigta/Dataset_spark/tmp/my-parquet-file.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5 > ORC 파일\n",
    "- 하둡워크로드를 위한 데이터 타입을 인식할 수 있는 컬럼 기반의 파일 포맷\n",
    "- 대규모 스트리밍 읽기에 최적화\n",
    "- 필요한 row를 빠르게 찾을 수 있음\n",
    "- parquet와 마찬가지로 옵션이 많이 없음\n",
    "<br><br>\n",
    "- parquet와 ORC의 차이\n",
    "    - parquet는 스파크에 최적화\n",
    "    - ORC는 하이브에 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ORC 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orcFile = spark.read.format(\"orc\")\\\n",
    "        .load(\"file:///home/ubuntu/ybigta/Dataset_spark/data/flight-data/orc/2010-summary.orc\")\n",
    "orcFile.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ORC 파일 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV파일을 ORC파일로 변환하기\n",
    "csvFile.write.format(\"orc\").mode(\"overwrite\")\\\n",
    ".save(\"file:///home/ubuntu/ybigta/Dataset_spark/tmp/my-orc-file.orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.6 > 텍스트 파일\n",
    "- 파일의 각 줄이 레코드가 됨\n",
    "- 기본 데이터 타입의 유연성을 활용할 수 있어 좋은 포맷!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 텍스트 파일 읽기\n",
    "- textFile 메서드는 파티션 수행 결과로 만들어진 디렉토리 명을 무시 :       왜 안되지...?pyspark는 안되나..?\n",
    "- text 메서드는 파티션 수행 결과로 만들어진 디렉토리 명을 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                rows|\n",
      "+--------------------+\n",
      "|[DEST_COUNTRY_NAM...|\n",
      "|[United States, R...|\n",
      "|[United States, I...|\n",
      "|[United States, I...|\n",
      "|[Egypt, United St...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 아마 scala에서 가능?\n",
    "# spark.read.textFile(\"file:///home/ubuntu/ybigta/Dataset_spark/data/flight-data/csv/2010-summary.csv\")\\\n",
    "#         .selectExpr(\"split(value, ',') as rows\").show()\n",
    "\n",
    "spark.read.text(\"file:///home/ubuntu/ybigta/Dataset_spark/data/flight-data/csv/2010-summary.csv\")\\\n",
    "        .selectExpr(\"split(value, ',') as rows\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 텍스트 파일 쓰기\n",
    "- 텍스트 파일을 쓸 때는 문자열 컬럼이 하나만 존재해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.limit(10).select(\"DEST_COUNTRY_NAME\").write\\\n",
    ".text(\"file:///home/ubuntu/ybigta/Dataset_spark/tmp/simple-text-file.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 파티셔닝 작업을 수행하면 더 많은 컬럼을 저장 가능...<pre> 무슨말 이게...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.limit(10).select(\"DEST_COUNTRY_NAME\", \"count\").write\\\n",
    ".partitionBy(\"count\").text(\"file:///home/ubuntu/ybigta/Dataset_spark/tmp/five-csv-files2py.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.7 > 고급 I/O 개념\n",
    "- 쓰기 작업 전 파티션 수 조절을 통해 병렬로 처리할 파일 수를 조절 가능\n",
    "    - 버켓팅\n",
    "    - 파티셔닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분할 가능한 파일 타입과 압축 방식\n",
    "- 특정 파일 포맷은 분할을 지원\n",
    "- 특정 압축 방식은 분할 압축을 지원\n",
    "##### 추천하는 파일 포맷과 압축 방식은 parquet, GZIP 압축방식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 병렬로 데이터 읽기\n",
    "- 다수의 파일이 존재하는 폴더를 읽을 때 개별 파일은 DataFrame의 파티션이 됨\n",
    "- 여러 익스큐터로 병렬로 파일을 읽음\n",
    "\n",
    "#### 병렬로 데이터 쓰기\n",
    "- 기본적으로 데이터 파티션 당 하나의 파일이 작성\n",
    "- 옵션에 지정된 파일명은 실제로는 다수의 파일을 가진 디렉토리 \n",
    "- 디렉토리에 파티션 당 하나의 파일로 데이터를 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5개의 CSV파일로 저장됨\n",
    "csvFile.repartition(5).write.format(\"csv\").mode(\"overwrite\").save(\"file:///home/ubuntu/ybigta/Dataset_spark/tmp/multiple.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파티셔닝\n",
    "- 파티셔닝된 디렉토리, 테이블에 파일을 쓸 때 디렉토리별로 컬럼 데이터를 인코딩해 저장\n",
    "- 전체 데이터셋을 스캔하지 않고 필요한 만큼만 읽을 수 있다.\n",
    "- 모든 파일 기반의 데이터소스에서 지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEST_COUNTRY_NAME 컬럼 value로 파티션이 이뤄짐\n",
    "csvFile.limit(10).write.mode(\"overwrite\").partitionBy(\"DEST_COUNTRY_NAME\")\\\n",
    ".save(\"file:///home/ubuntu/ybigta/Dataset_spark/tmp/partitioned-files.parquet\")\n",
    "\n",
    "# 결과적으로 각 폴터는 조건절을 폴더명으로 사용 > 조건을 만족하는 데이터가 저장된 파케이 파일을 갖게됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 버켓팅\n",
    "- 동일한 버킷ID를 가진 데이터가 하나의 물리적 파티션으로 묶임\n",
    "- 조인, 집계, 읽을 시 데이터 셔플을 피할 수 있음\n",
    "- 스파크 관리 테이블에서만 사용가능\n",
    "<br><br>\n",
    "- 왜 버켓팅?\n",
    "   - 특정 컬럼 기준으로 파티셔닝할 시 수억개의 디렉토리가 생길수도...\n",
    "   - 버켓 단위로 모아 일정 수의 디렉토리를 유지!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scala! hive!\n",
    "# val numberBuckets = 10\n",
    "# val columnToBucketBy = \"count\"\n",
    "\n",
    "# csvFile.limit(10).write.mode(\"overwrite\").bucketBy(numberBuckets, columnToBucketBy)\\\n",
    "# .saveAsTable(\"file:///home/ubuntu/ybigta/Dataset_spark/tmp/bucketedFiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.9 > 파일크기관리\n",
    "- 작은파일이 너무 많으면 메타데이터 관리에 부하가 생김\n",
    "- 큰 파일이 너무 많아도 전체 데이터를 읽어야하기에 부하가 생김\n",
    "- 결과 파일을 최적의 크기로 제한할 수 있는 새로운 기능!!!\n",
    "    - maxRecordsPerFile : 각 파일에 기록될 레코드 수를 조절할 수 있다. \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.option(\"maxRecordPerFile\", 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
